{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20f786e",
   "metadata": {},
   "source": [
    "# Домашнее задание № 2. Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf72d19",
   "metadata": {},
   "source": [
    "## Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a045e99",
   "metadata": {},
   "source": [
    "У векторайзеров в sklearn есть встроенная токенизация на регулярных выражениях. Найдите способо заменить её на кастомную токенизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b4d453",
   "metadata": {},
   "source": [
    "Обучите векторайзер с дефолтной токенизацией и с токенизацией razdel.tokenize. Обучите классификатор (любой) с каждым из векторизаторов. Сравните метрики и выберете победителя. \n",
    "\n",
    "(в вашей тетрадке должен быть код обучения и все метрики; если вы сдаете в .py файлах то сохраните полученные метрики в отдельном файле или в комментариях)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129c4d2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:19:38.944428Z",
     "start_time": "2023-06-23T11:19:30.986119Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbffbbc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:22:45.673619Z",
     "start_time": "2023-06-23T11:22:37.556247Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4314de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:39.817498Z",
     "start_time": "2023-06-23T11:39:39.698485Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/alexandradolidze/Desktop/compling/labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d2477b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:40.048947Z",
     "start_time": "2023-06-23T11:39:40.038887Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.66514\n",
       "1.0    0.33486\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a07af34c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:40.516310Z",
     "start_time": "2023-06-23T11:39:40.502188Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, shuffle=True)\n",
    "train.reset_index(inplace=True)\n",
    "test.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57a9a9fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:41.170934Z",
     "start_time": "2023-06-23T11:39:41.166134Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = train.toxic.values\n",
    "y_test = test.toxic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5b50abc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:41.628639Z",
     "start_time": "2023-06-23T11:39:41.624542Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e05806a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:42.611775Z",
     "start_time": "2023-06-23T11:39:42.044692Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(train.comment)\n",
    "X_test = vectorizer.transform(test.comment) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8206faf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:42:05.966635Z",
     "start_time": "2023-06-23T11:42:04.983793Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8675    0.9361    0.9005       986\n",
      "         1.0     0.8333    0.6908    0.7554       456\n",
      "\n",
      "    accuracy                         0.8585      1442\n",
      "   macro avg     0.8504    0.8134    0.8279      1442\n",
      "weighted avg     0.8567    0.8585    0.8546      1442\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592d6be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:34:59.045823Z",
     "start_time": "2023-06-23T11:34:59.038621Z"
    }
   },
   "source": [
    "С токенизацией razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b25ef42f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:39:47.811722Z",
     "start_time": "2023-06-23T11:39:44.366781Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(tokenizer=lambda x: [w.text for w in tokenize(x)])\n",
    "X_train_razdel = vectorizer.fit_transform(train.comment)\n",
    "X_test_razdel = vectorizer.transform(test.comment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ed0038b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:42:00.261822Z",
     "start_time": "2023-06-23T11:41:59.266457Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.8701    0.9371    0.9023       986\n",
      "         1.0     0.8368    0.6974    0.7608       456\n",
      "\n",
      "    accuracy                         0.8613      1442\n",
      "   macro avg     0.8534    0.8172    0.8316      1442\n",
      "weighted avg     0.8596    0.8613    0.8576      1442\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model_razdel = LogisticRegression()\n",
    "model_razdel.fit(X_train_razdel, y_train)\n",
    "\n",
    "y_pred_razdel = model_razdel.predict(X_test_razdel)\n",
    "\n",
    "print(classification_report(y_test, y_pred_razdel, digits = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afee5953",
   "metadata": {},
   "source": [
    "Модель, обучавшаяся на данных, токенезированных razdel, дает немного (совсем немного) лучшие результаты, показывая более высокие f1-score и accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c896c",
   "metadata": {},
   "source": [
    "## Задание 2 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcd27a3",
   "metadata": {},
   "source": [
    "Преобразуйте таблицу с абсолютными частотностями в семинарской тетрадке в таблицу с tfidf значениями. (Таблица - https://i.ibb.co/r5Nc2HC/abs-bow.jpg) Формула tfidf есть в семинаре на картнике с пояснениями на английском. \n",
    "Считать нужно в питоне. Формат итоговой таблицы может быть любым, главное, чтобы был код и можно было воспроизвести вычисления. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "90b51e4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-23T11:45:03.691649Z",
     "start_time": "2023-06-23T11:45:03.649216Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = ['я и ты', 'ты и я', 'я, я и только я', 'только не я', 'он']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c2f2f9ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T07:20:45.833877Z",
     "start_time": "2023-06-24T07:20:45.761677Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m word_set \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 4\u001b[0m     x \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;28;01mfor\u001b[39;00m  i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mword_tokenize\u001b[49m(sent) \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m.\u001b[39misalpha()]\n\u001b[1;32m      5\u001b[0m     sentences\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m x:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "word_set = []\n",
    " \n",
    "for sent in sentences:\n",
    "    x = [i.lower() for  i in word_tokenize(sent) if i.isalpha()]\n",
    "    sentences.append(x)\n",
    "    for word in x:\n",
    "        if word not in word_set:\n",
    "            word_set.append(word)\n",
    "\n",
    "word_set = set(word_set)\n",
    "total_documents = len(sentences)\n",
    " \n",
    "index_dict = {} \n",
    "i = 0\n",
    "for word in word_set:\n",
    "    index_dict[word] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad53bafb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T07:20:46.370726Z",
     "start_time": "2023-06-24T07:20:46.319885Z"
    }
   },
   "outputs": [],
   "source": [
    "def count_dict(sentences):\n",
    "    word_count = {}\n",
    "    for word in word_set:\n",
    "        word_count[word] = 0\n",
    "        for sent in sentences:\n",
    "            if word in sent:\n",
    "                word_count[word] += 1\n",
    "    return word_count\n",
    " \n",
    "word_count = count_dict(sentences)\n",
    "\n",
    "def termfreq(document, word):\n",
    "    N = len(document)\n",
    "    occurance = len([token for token in document if token == word])\n",
    "    return occurance/N\n",
    "\n",
    "def inverse_doc_freq(word):\n",
    "    try:\n",
    "        word_occurance = word_count[word] + 1\n",
    "    except:\n",
    "        word_occurance = 1\n",
    "    return np.log(total_documents/word_occurance)\n",
    "\n",
    "\n",
    "def tf_idf(sentence):\n",
    "    tf_idf_vec = np.zeros((len(word_set),))\n",
    "    for word in sentence:\n",
    "        tf = termfreq(sentence,word)\n",
    "        idf = inverse_doc_freq(word)\n",
    "         \n",
    "        value = tf*idf\n",
    "        tf_idf_vec[index_dict[word]] = value \n",
    "    return tf_idf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32ab3b27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-24T07:20:47.071741Z",
     "start_time": "2023-06-24T07:20:46.986410Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m vectors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 4\u001b[0m     vec \u001b[38;5;241m=\u001b[39m \u001b[43mtf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     vectors\u001b[38;5;241m.\u001b[39mappend(vec)\n",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36mtf_idf\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtf_idf\u001b[39m(sentence):\n\u001b[0;32m---> 26\u001b[0m     tf_idf_vec \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(word_set),))\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[1;32m     28\u001b[0m         tf \u001b[38;5;241m=\u001b[39m termfreq(sentence,word)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#TF-IDF Encoded text corpus\n",
    "vectors = []\n",
    "for sent in sentences:\n",
    "    vec = tf_idf(sent)\n",
    "    vectors.append(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b9076e",
   "metadata": {},
   "source": [
    "## Задание 3 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e25357",
   "metadata": {},
   "source": [
    "Обучите 2 любых разных классификатора из семинара. Предскажите токсичность для текстов из тестовой выборки (используйте одну и ту же выборку для обоих классификаторов) и найдите 10 самых токсичных для каждого из классификаторов. Сравните получаемые тексты - какие тексты совпадают, какие отличаются, правда ли тексты токсичные?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de962ad",
   "metadata": {},
   "source": [
    "Требования к моделям:   \n",
    "а) один классификатор должен использовать CountVectorizer, другой TfidfVectorizer  \n",
    "б) у векторазера должны быть вручную заданы как минимум 5 параметров (можно ставить разные параметры tfidfvectorizer и countvectorizer)  \n",
    "в) у классификатора должно быть задано вручную как минимум 2 параметра (по возможности)  \n",
    "г)  f1 мера каждого из классификаторов должна быть минимум 0.75  \n",
    "\n",
    "*random_seed не считается за параметр"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a5471",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bd58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90311fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f228c3e",
   "metadata": {},
   "source": [
    "## *Задание 4 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566929b7",
   "metadata": {},
   "source": [
    "Для классификаторов LogisticRegression и Random Forest найдите способ извлечь важность признаков для предсказания токсичного класса. Сопоставьте полученные числа со словами (или нграммами) в словаре и найдите топ - 5 \"токсичных\" слов для каждого из классификаторов. \n",
    "\n",
    "Важное требование: в топе не должно быть стоп-слов. Для этого вам нужно будет правильным образом настроить векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f86878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15af4e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
